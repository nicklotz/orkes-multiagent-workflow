{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Building a Multi-Agent Customer Support Triage System with Orkes Conductor\n\nThis notebook demonstrates the technical implementation of a multi-agent system using Orkes Conductor to handle customer support ticket triage.\n\n## Architecture Overview\n\nOur system consists of three specialized AI agents:\n1. **Classifier Agent**: Analyzes ticket content to determine category, sentiment, and urgency\n2. **Knowledge Agent**: Searches internal documentation for relevant solutions\n3. **Escalation Agent**: Routes unresolved or high-priority tickets to human agents\n\nWe'll also implement a custom worker task to demonstrate how external services can integrate with the workflow.\n\n---\n\n## Prerequisites: Environment Setup\n\nThis notebook uses a dedicated conda environment to ensure clean dependency management. Follow the setup instructions below before running the notebook.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Building a Multi-Agent Customer Support System with Orkes Conductor\n\nYou're about to build something powerful: a multi-agent AI system that handles customer support tickets automatically. Think of it as assembling a specialized team where each member has one job and does it exceptionally well.\n\n## What We're Building\n\nThree AI agents working together:\n1. **Classifier Agent**: Your intake specialist\u2014reads every ticket and figures out what's urgent, what category it falls into, and how the customer feels about it\n2. **Knowledge Agent**: Your documentation expert\u2014searches your knowledge base and suggests solutions with confidence scores\n3. **Escalation Agent**: Your decision maker\u2014routes tickets to humans when needed, auto-resolves when confident\n\nPlus a **custom worker task** that shows you how to integrate external services (Slack, email, PagerDuty) into the workflow.\n\nThe real lesson here isn't just about AI agents. It's about **orchestration**\u2014how you coordinate multiple specialized components into a system that's more reliable than any single agent could ever be.\n\n---\n\n## Before You Start: Environment Setup\n\nThis notebook uses a dedicated conda environment. Why? Because dependency conflicts are painful, and you want a clean slate. If you've ever had one project break another because of a package upgrade, you know why isolated environments matter.\n\nFollow the setup below, then come back and run the rest of the notebook.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Step 0: Check for Conda\n\nFirst things first\u2014let's make sure you have conda installed. If not, we'll point you to the installation instructions.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Step 0.5: Create Your Environment\n\nRun the cells below to create a conda environment named `orkes-multiagent`. This gives you an isolated Python environment with exactly the dependencies you need\u2014nothing more, nothing less.\n\n**Important**: After creating the environment, you need to tell Jupyter to use it:\n- **Jupyter Notebook**: `Kernel` \u2192 `Change Kernel` \u2192 `orkes-multiagent`\n- **JupyterLab**: Click the kernel name (top-right) \u2192 Select `orkes-multiagent`\n- **VS Code**: Click the kernel selector \u2192 Choose `orkes-multiagent`\n\nThink of this like switching which Python installation you're using\u2014it's the same notebook, different environment.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Install ipykernel in the environment and register it as a Jupyter kernel\n!conda run -n orkes-multiagent pip install ipykernel\n!conda run -n orkes-multiagent python -m ipykernel install --user --name=orkes-multiagent --display-name=\"Python (orkes-multiagent)\"\n\nprint(\"\\n\u2705 Environment created successfully!\")\nprint(\"\\n\u26a0\ufe0f  IMPORTANT: Now you need to switch to the 'orkes-multiagent' kernel:\")\nprint(\"   1. In Jupyter: Kernel \u2192 Change Kernel \u2192 Python (orkes-multiagent)\")\nprint(\"   2. In VS Code: Click kernel selector (top right) \u2192 Python (orkes-multiagent)\")\nprint(\"   3. Then continue with Step 1 below\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "## Step 1: Install Dependencies\n\n**Before running this**: Make sure you switched to the `orkes-multiagent` kernel (see instructions above). Otherwise, you'll install packages in the wrong environment and wonder why nothing works.\n\nThis installs the Orkes Conductor Python SDK, OpenAI client, and a few utilities."
    },
    {
      "cell_type": "code",
      "source": "# Verify you're using the correct environment\nimport sys\nprint(f\"Python executable: {sys.executable}\")\nprint(f\"Python version: {sys.version}\")\n\nif \"orkes-multiagent\" in sys.executable or \"orkes-multiagent\" in sys.prefix:\n    print(\"\\n\u2705 You're using the orkes-multiagent environment!\")\nelse:\n    print(\"\\n\u26a0\ufe0f  WARNING: You may not be using the orkes-multiagent environment.\")\n    print(\"   Please switch kernels before continuing.\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "## Step 2: Connect to Orkes Conductor\n\nTime to configure your connection to Orkes. You'll need credentials from your Orkes account (sign up for free at orkes.io if you haven't already).\n\nYour credentials go in a `.env` file\u2014never hardcode API keys in notebooks or scripts. This way, you can share code without sharing secrets."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "## Step 3: Build the Classifier Agent\n\nHere's where it gets interesting. The Classifier Agent's job is simple: read a ticket and extract structured information\u2014category, sentiment, urgency.\n\nNotice what you're *not* doing: you're not calling OpenAI's API directly, not managing retries, not handling timeouts. Conductor does all of that. You just define what the agent should do."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "## Step 4: Build the Knowledge Agent\n\nThe Knowledge Agent builds on what the Classifier learned. See the `${classify_ticket.output.result.category}` syntax? That's data flowing from the first agent into the second.\n\nThis is multi-agent coordination in action\u2014each agent does its job and passes the baton to the next. The workflow becomes a pipeline where intelligence accumulates."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "## Step 5: Create a Custom Worker Task\n\nHere's where things get practical. LLM agents are great for reasoning, but real systems need to *do* things\u2014send a Slack message, create a Jira ticket, trigger PagerDuty.\n\nThat's what worker tasks are for. They're custom code that runs outside Conductor but is orchestrated by it. The worker actively polls Conductor asking \"got any work for me?\" When a task is ready, the worker executes it and reports back.\n\nThis polling model gives you decoupling, scalability, and reliability. Your worker can live anywhere\u2014different server, different cloud, even different language."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "## Step 6: Build the Escalation Agent\n\nThe Escalation Agent is your decision maker. It looks at the ticket urgency, the category, and how confident the Knowledge Agent was, then decides: escalate to a human or auto-resolve?\n\nThis is another LLM-based agent, but its job is judgment\u2014not classification or search."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "## Step 7: Wire Everything Together\n\nNow comes the satisfying part\u2014assembling all the pieces into a complete workflow.\n\nThe `>>` operator chains tasks together. Conductor takes this definition and manages execution order, data flow, retries, and observability. You get production-ready orchestration without writing any of that plumbing yourself."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "## Step 8: Register the Worker (Optional)\n\nIn production, workers run as separate processes\u2014typically containerized services that poll Conductor continuously. For this demo, we're skipping the actual worker startup (that's what `task_handler.start_processes()` would do).\n\nThe workflow will still run, but the notification task will wait for a worker that isn't there. That's okay\u2014it demonstrates how Conductor orchestrates heterogeneous tasks (LLM agents + custom workers)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "## Step 9: Test It Out\n\nLet's run this thing. Three sample tickets\u2014login issues, a critical outage, and a billing question. Each one will flow through the entire workflow: Classifier \u2192 Knowledge \u2192 Escalation \u2192 Notification.\n\nThe workflows execute asynchronously in Orkes. You submit them here, but they run in the cloud. Check the Conductor UI to watch them in real-time."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "## Step 10: Monitor and Debug\n\nConductor gives you visibility into every workflow execution. Use this function to inspect a workflow\u2014you'll see every task, its status, inputs, outputs, and how long it took.\n\nWhen something breaks (and it will), this is how you debug. No more black boxes."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## What You Just Built\n\nYou built a production-ready multi-agent system. Here's what makes it powerful:\n\n**Multi-Agent Orchestration**: Three specialized AI agents (Classifier, Knowledge, Escalation) working together\u2014each doing one job exceptionally well.\n\n**Custom Worker Integration**: A worker task that shows how to integrate external services (Slack, PagerDuty) into AI workflows.\n\n**Production Patterns**: Built-in retry logic, error handling, state management, and observability. You don't write this yourself\u2014Conductor provides it.\n\n**Workflow Composition**: Easy to test, easy to modify, easy to scale. Want to add a fourth agent? Just add it to the workflow chain.\n\n### Why This Matters\n\n**Separation of Concerns**: Change one agent without breaking others. Deploy the new Classifier, and your workflow automatically uses it.\n\n**Independent Scaling**: Maybe your Knowledge Agent needs GPUs. The Classifier runs fine on CPUs. With Conductor, scale each component independently.\n\n**Observability**: When a ticket gets misrouted, you see exactly what every agent returned. No guessing.\n\n**Flexibility**: Add duplicate detection, human-in-the-loop approval, or A/B testing different prompts\u2014all workflow changes, not agent rewrites.\n\n### Next Steps\n\n1. Connect your actual LLM provider and knowledge base\n2. Implement real notification channels (Slack, email, PagerDuty)\n3. Add error handling and retry policies\n4. Deploy workers as containerized services (see README for Docker/Kubernetes examples)\n5. Set up monitoring and alerting\n\nThe future isn't bigger AI models. It's better orchestration of specialized agents. You just built the foundation for that."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}