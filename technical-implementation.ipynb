{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Multi-Agent Customer Support Triage System with Orkes Conductor\n",
    "\n",
    "This notebook demonstrates the technical implementation of a multi-agent system using Orkes Conductor to handle customer support ticket triage.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "Our system consists of three specialized AI agents:\n",
    "1. **Classifier Agent**: Analyzes ticket content to determine category, sentiment, and urgency\n",
    "2. **Knowledge Agent**: Searches internal documentation for relevant solutions\n",
    "3. **Escalation Agent**: Routes unresolved or high-priority tickets to human agents\n",
    "\n",
    "We'll also implement a custom worker task to demonstrate how external services can integrate with the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Building a Multi-Agent Customer Support Triage System with Orkes Conductor\n\nThis notebook demonstrates the technical implementation of a multi-agent system using Orkes Conductor to handle customer support ticket triage.\n\n## Architecture Overview\n\nOur system consists of three specialized AI agents:\n1. **Classifier Agent**: Analyzes ticket content to determine category, sentiment, and urgency\n2. **Knowledge Agent**: Searches internal documentation for relevant solutions\n3. **Escalation Agent**: Routes unresolved or high-priority tickets to human agents\n\nWe'll also implement a custom worker task to demonstrate how external services can integrate with the workflow.\n\n---\n\n## Prerequisites: Environment Setup\n\nThis notebook uses a dedicated conda environment to ensure clean dependency management. Follow the setup instructions below before running the notebook.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Step 0: Check and Install Conda\n\nFirst, let's check if conda is installed. If not, we'll provide instructions to install it.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import subprocess\nimport sys\nimport os\n\ndef check_conda():\n    \"\"\"Check if conda is installed\"\"\"\n    try:\n        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n        if result.returncode == 0:\n            print(f\"âœ… Conda is installed: {result.stdout.strip()}\")\n            return True\n        else:\n            print(\"âŒ Conda not found\")\n            return False\n    except FileNotFoundError:\n        print(\"âŒ Conda not found\")\n        return False\n\nif not check_conda():\n    print(\"\\nðŸ“¦ Conda Installation Instructions:\")\n    print(\"\\nOption 1 - Miniconda (Recommended, lightweight):\")\n    print(\"  macOS/Linux: https://docs.conda.io/en/latest/miniconda.html\")\n    print(\"  Run: bash Miniconda3-latest-MacOSX-x86_64.sh\")\n    print(\"\\nOption 2 - Anaconda (Full distribution):\")\n    print(\"  Download from: https://www.anaconda.com/download\")\n    print(\"\\nAfter installation, restart your terminal and this notebook.\")\nelse:\n    print(\"\\nâœ… Ready to proceed with environment setup!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 0.5: Create Conda Environment\n\nRun the cells below to create and configure a dedicated conda environment named `orkes-multiagent`.\n\n**Note**: After creating the environment, you'll need to select it as your Jupyter kernel:\n- In Jupyter Notebook: `Kernel` â†’ `Change Kernel` â†’ `orkes-multiagent`\n- In JupyterLab: Click the kernel name in top-right â†’ Select `orkes-multiagent`\n- In VS Code: Click the kernel selector â†’ Choose `orkes-multiagent`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create conda environment\n!conda create -n orkes-multiagent python=3.11 -y",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Install ipykernel in the environment and register it as a Jupyter kernel\n!conda run -n orkes-multiagent pip install ipykernel\n!conda run -n orkes-multiagent python -m ipykernel install --user --name=orkes-multiagent --display-name=\"Python (orkes-multiagent)\"\n\nprint(\"\\nâœ… Environment created successfully!\")\nprint(\"\\nâš ï¸  IMPORTANT: Now you need to switch to the 'orkes-multiagent' kernel:\")\nprint(\"   1. In Jupyter: Kernel â†’ Change Kernel â†’ Python (orkes-multiagent)\")\nprint(\"   2. In VS Code: Click kernel selector (top right) â†’ Python (orkes-multiagent)\")\nprint(\"   3. Then continue with Step 1 below\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Install Required Dependencies\n\n**Before running this cell**, make sure you've switched to the `orkes-multiagent` kernel (see instructions above).\n\nThis cell installs the Orkes Conductor Python SDK and other dependencies."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install conductor-python openai requests python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Verify you're using the correct environment\nimport sys\nprint(f\"Python executable: {sys.executable}\")\nprint(f\"Python version: {sys.version}\")\n\nif \"orkes-multiagent\" in sys.executable or \"orkes-multiagent\" in sys.prefix:\n    print(\"\\nâœ… You're using the orkes-multiagent environment!\")\nelse:\n    print(\"\\nâš ï¸  WARNING: You may not be using the orkes-multiagent environment.\")\n    print(\"   Please switch kernels before continuing.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Configure Conductor Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom dotenv import load_dotenv\nfrom conductor.client.configuration.configuration import Configuration\nfrom conductor.shared.configuration.settings.authentication_settings import AuthenticationSettings\nfrom conductor.client.orkes_clients import OrkesClients\nfrom conductor.client.http.models import Task, WorkflowDef, TaskDef\nfrom conductor.client.worker.worker_task import WorkerTask\nfrom conductor.client.workflow.conductor_workflow import ConductorWorkflow\nfrom conductor.client.workflow.task.llm_tasks.llm_chat_complete import LlmChatComplete, ChatMessage\nfrom conductor.client.workflow.task.simple_task import SimpleTask\nfrom conductor.client.workflow.task.switch_task import SwitchTask\nfrom conductor.client.workflow.executor.workflow_executor import WorkflowExecutor\nimport json\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Get credentials from environment\nkey_id = os.getenv(\"ORKES_KEY_ID\")\nkey_secret = os.getenv(\"ORKES_KEY_SECRET\")\nserver_url = os.getenv(\"ORKES_SERVER_URL\", \"https://play.orkes.io/api\")\n\n# Create authentication settings\nauth_settings = AuthenticationSettings(\n    key_id=key_id,\n    key_secret=key_secret\n)\n\n# Configure your Orkes Conductor instance\nconfiguration = Configuration(\n    server_api_url=server_url,\n    authentication_settings=auth_settings\n)\n\nclients = OrkesClients(configuration)\nworkflow_executor = WorkflowExecutor(configuration)\n\nprint(f\"âœ… Connected to: {configuration.host}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Classifier Agent\n",
    "\n",
    "The Classifier Agent uses an LLM to analyze incoming tickets and extract key information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_classifier_agent():\n    \"\"\"\n    Creates the classifier agent task that analyzes ticket content\n    Returns: category, sentiment, urgency\n    \"\"\"\n    \n    return LlmChatComplete(\n        task_ref_name=\"classify_ticket\",\n        llm_provider=\"openai\",  # Change to your integration name (e.g., \"nl-openai\")\n        model=\"gpt-4\",\n        messages=[\n            ChatMessage(role=\"system\", message=\"You are a customer support ticket classifier. Analyze tickets and provide category (billing/technical/account/general), sentiment (positive/neutral/negative/angry), and urgency (low/medium/high/critical) in JSON format with a reasoning field.\"),\n            ChatMessage(role=\"user\", message=\"${workflow.input.ticket_content}\")\n        ]\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the Knowledge Agent\n",
    "\n",
    "The Knowledge Agent searches internal documentation and past resolutions to find potential solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_knowledge_agent():\n    \"\"\"\n    Creates the knowledge search agent that looks for solutions\n    in documentation, FAQs, and past tickets\n    \"\"\"\n    \n    return LlmChatComplete(\n        task_ref_name=\"search_knowledge\",\n        llm_provider=\"openai\",  # Change to your integration name (e.g., \"nl-openai\")\n        model=\"gpt-4\",\n        messages=[\n            ChatMessage(role=\"system\", message=\"You are a knowledge base expert. Provide KB articles, resolution steps, confidence level (0-100), and a suggested customer response in JSON format.\"),\n            ChatMessage(role=\"user\", message=\"Ticket: ${workflow.input.ticket_content}. Category: ${classify_ticket.output.result.category}. Urgency: ${classify_ticket.output.result.urgency}\")\n        ]\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Custom Worker Task for Notification\n",
    "\n",
    "This worker task demonstrates how to implement custom business logic that runs outside of Conductor but is orchestrated by it. This worker will handle sending notifications to Slack or email when tickets are escalated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conductor.client.worker.worker_task import WorkerTask\n",
    "\n",
    "@WorkerTask(task_definition_name='send_escalation_notification', domain='support', poll_interval=0.5)\n",
    "def send_notification(task_input):\n",
    "    \"\"\"\n",
    "    Custom worker that sends notifications when tickets are escalated.\n",
    "    This worker polls Conductor for tasks and executes them.\n",
    "    \n",
    "    Args:\n",
    "        task_input: Dictionary containing ticket details and escalation info\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with notification status\n",
    "    \"\"\"\n",
    "    ticket_id = task_input.get('ticket_id')\n",
    "    urgency = task_input.get('urgency')\n",
    "    category = task_input.get('category')\n",
    "    assigned_to = task_input.get('assigned_to', 'support-team')\n",
    "    \n",
    "    print(f\"ðŸš¨ Escalating ticket {ticket_id}\")\n",
    "    print(f\"   Category: {category}\")\n",
    "    print(f\"   Urgency: {urgency}\")\n",
    "    print(f\"   Assigned to: {assigned_to}\")\n",
    "    \n",
    "    # In production, you would send actual notifications here:\n",
    "    # - Slack webhook\n",
    "    # - Email via SendGrid/SES\n",
    "    # - PagerDuty for critical issues\n",
    "    # - Update ticketing system (Zendesk, Jira, etc.)\n",
    "    \n",
    "    notification_channels = []\n",
    "    \n",
    "    if urgency in ['critical', 'high']:\n",
    "        notification_channels.extend(['slack', 'email', 'pagerduty'])\n",
    "    else:\n",
    "        notification_channels.append('email')\n",
    "    \n",
    "    # Simulate sending notifications\n",
    "    results = {\n",
    "        'notification_sent': True,\n",
    "        'channels': notification_channels,\n",
    "        'timestamp': '2025-01-15T10:30:00Z',\n",
    "        'assigned_team': assigned_to\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define the Escalation Logic\n",
    "\n",
    "Based on the knowledge agent's confidence and the ticket urgency, we'll decide whether to auto-resolve or escalate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_escalation_decision():\n    \"\"\"\n    Creates a switch task that routes tickets based on confidence and urgency\n    \"\"\"\n    # This is a simple task that evaluates escalation criteria\n    escalation_eval = SimpleTask(\n        task_def_name=\"evaluate_escalation\",\n        task_reference_name=\"eval_escalation\"\n    )\n    \n    # Worker task for sending notifications\n    notification_task = SimpleTask(\n        task_def_name=\"send_escalation_notification\",\n        task_reference_name=\"notify_team\"\n    )\n    \n    # Set input parameters after creating the task\n    notification_task.input_parameters = {\n        'ticket_id': '${workflow.input.ticket_id}',\n        'urgency': '${classify_ticket.output.result.urgency}',\n        'category': '${classify_ticket.output.result.category}',\n        'assigned_to': '${eval_escalation.output.assigned_team}'\n    }\n    \n    return escalation_eval, notification_task"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Build the Complete Workflow\n",
    "\n",
    "Now we'll assemble all agents into a cohesive workflow that orchestrates the entire triage process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_support_triage_workflow():\n    \"\"\"\n    Creates the complete multi-agent support triage workflow\n    \"\"\"\n    workflow = ConductorWorkflow(\n        name=\"customer_support_triage\",\n        version=1,\n        executor=workflow_executor\n    )\n    \n    # Add all tasks in sequence\n    classifier = create_classifier_agent()\n    knowledge = create_knowledge_agent()\n    # Note: Escalation tasks commented out for initial testing\n    # Uncomment once you've verified the LLM agents work correctly\n    # escalation_eval, notification = create_escalation_decision()\n    \n    # Build the workflow chain\n    workflow >> classifier >> knowledge\n    # workflow >> escalation_eval >> notification\n    \n    return workflow\n\n# Create and register the workflow\nsupport_workflow = create_support_triage_workflow()\nprint(\"âœ… Workflow created: customer_support_triage\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Register the Worker\n",
    "\n",
    "The worker needs to be running to poll for tasks. In production, this would be a separate service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conductor.client.automator.task_handler import TaskHandler\n",
    "\n",
    "# Create task handler and register the worker\n",
    "task_handler = TaskHandler(\n",
    "    workers=[send_notification],\n",
    "    configuration=configuration,\n",
    "    scan_for_annotated_workers=False\n",
    ")\n",
    "\n",
    "# Start polling for tasks (in production, this runs as a daemon)\n",
    "# task_handler.start_processes()\n",
    "\n",
    "print(\"âœ… Worker registered and ready to process tasks\")\n",
    "print(\"   Task Definition: send_escalation_notification\")\n",
    "print(\"   Domain: support\")\n",
    "print(\"   Poll Interval: 0.5s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test the Workflow\n",
    "\n",
    "Let's test our multi-agent system with sample support tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test tickets\n",
    "test_tickets = [\n",
    "    {\n",
    "        \"ticket_id\": \"TKT-001\",\n",
    "        \"ticket_content\": \"I can't log into my account. I've tried resetting my password but I'm not receiving the email.\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket_id\": \"TKT-002\",\n",
    "        \"ticket_content\": \"URGENT: Our production system is down and we're losing money every minute. This is critical!\"\n",
    "    },\n",
    "    {\n",
    "        \"ticket_id\": \"TKT-003\",\n",
    "        \"ticket_content\": \"Hi, I was charged twice for my subscription this month. Can you help?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute workflows for each ticket\n",
    "for ticket in test_tickets:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {ticket['ticket_id']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Execute the workflow\n",
    "    workflow_id = support_workflow.execute(\n",
    "        workflow_input=ticket\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… Workflow started with ID: {workflow_id}\")\n",
    "    print(f\"   View in Conductor UI: https://play.orkes.io/execution/{workflow_id}\")\n",
    "    \n",
    "    # In production, you would monitor workflow status:\n",
    "    # workflow_status = workflow_executor.get_workflow(workflow_id)\n",
    "    # print(f\"   Status: {workflow_status.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Monitor and Debug\n",
    "\n",
    "Orkes Conductor provides powerful monitoring capabilities. Here's how to check workflow execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_workflow(workflow_id):\n",
    "    \"\"\"\n",
    "    Retrieve and display workflow execution details\n",
    "    \"\"\"\n",
    "    workflow_client = clients.get_workflow_client()\n",
    "    execution = workflow_client.get_workflow(workflow_id, include_tasks=True)\n",
    "    \n",
    "    print(f\"\\nWorkflow Status: {execution.status}\")\n",
    "    print(f\"\\nTask Execution Details:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for task in execution.tasks:\n",
    "        print(f\"\\nðŸ“‹ {task.task_type}: {task.reference_task_name}\")\n",
    "        print(f\"   Status: {task.status}\")\n",
    "        print(f\"   Start Time: {task.start_time}\")\n",
    "        print(f\"   End Time: {task.end_time}\")\n",
    "        \n",
    "        if task.output_data:\n",
    "            print(f\"   Output: {json.dumps(task.output_data, indent=2)[:200]}...\")\n",
    "    \n",
    "    return execution\n",
    "\n",
    "# Example usage:\n",
    "# execution_details = monitor_workflow(workflow_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This implementation demonstrates:\n",
    "\n",
    "1. **Multi-Agent Orchestration**: Three specialized AI agents (Classifier, Knowledge, Escalation) working together\n",
    "2. **Custom Worker Integration**: A worker task that polls Conductor and executes external business logic\n",
    "3. **Workflow Composition**: Using Conductor's SDK to build complex workflows with conditional logic\n",
    "4. **Monitoring & Observability**: Built-in capabilities to track workflow execution\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Separation of Concerns**: Each agent has a single, well-defined responsibility\n",
    "- **Scalability**: Workers can be scaled independently based on load\n",
    "- **Reliability**: Conductor handles retries, error handling, and task persistence\n",
    "- **Flexibility**: Easy to add new agents or modify the workflow without changing agent code\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Integrate with your actual LLM provider and knowledge base\n",
    "2. Add error handling and retry policies\n",
    "3. Implement real notification channels (Slack, Email, PagerDuty)\n",
    "4. Set up monitoring and alerting\n",
    "5. Deploy workers as containerized services\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}